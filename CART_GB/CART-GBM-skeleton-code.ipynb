{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_svm_train = np.loadtxt('svm-train.txt')\n",
    "data_svm_test = np.loadtxt('svm-test.txt')\n",
    "\n",
    "x_svm_train, y_svm_train = data_svm_train[:, 0: 2], data_svm_train[:, 2].reshape(-1, 1)\n",
    "x_svm_test, y_svm_test = data_svm_test[:, 0: 2], data_svm_test[:, 2].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select positive target and negative target\n",
    "positive_index = np.where(y_svm_train>0)[0]\n",
    "x_positive = x_svm_train[positive_index]\n",
    "\n",
    "negative_index = np.where(y_svm_train<=0)[0]\n",
    "x_negative = x_svm_train[negative_index]\n",
    "\n",
    "#plot the predictions for the training set\n",
    "figsize = plt.figaspect(1)\n",
    "f, (ax) = plt.subplots(1, 1, figsize=figsize) \n",
    "\n",
    "pluses = ax.scatter (x_positive[:,0], x_positive[:,1], marker='+', c='r', label = '+1 labels for training set')\n",
    "minuses = ax.scatter (x_negative[:,0], x_negative[:,1], marker=r'$-$', c='b', label = '-1 labels for training set')\n",
    "\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=11)\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=11)\n",
    "ax.set_title('Training set size = %s'% len(data_svm_train), fontsize=9)  \n",
    "ax.axis('tight')\n",
    "ax.legend(handles=[pluses, minuses], fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change target to 0-1 label\n",
    "y_svm_train_label = np.array(list(map(lambda x: 1 if x > 0 else 0, y_svm_train))).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DTree():\n",
    "    '''\n",
    "    Decision Tree class\n",
    "    \n",
    "    :attribute train: a numpy array of training data in this node\n",
    "    :attribute value: value estimated by this node\n",
    "    :attribute split_score_function: function used for compute splitting criterion\n",
    "    :attribute leaf_value_estimator: function used for estimating leaf value\n",
    "    :attribute is_leaf(boolen): indicator of leaf node\n",
    "    :attribute depth: depth of this node\n",
    "    :attribute min_sample: threshold of minimum sample for splitting\n",
    "    :attribute max_depth: threshold of maximum depth of decision tree\n",
    "    :attribute split_id: feature used for splitting this node\n",
    "    :attribute split_value: value used for splitting this node\n",
    "    :attribute left: DTree_classifier object\n",
    "    :attribute right: DTree_classifier object\n",
    "    \n",
    "    :method split_node: splitting internal node\n",
    "    :method split_tree: searching split_id and split_value based on split_score_function\n",
    "    :method predict: predicting leaf value based on leaf_value_estimator\n",
    "    '''\n",
    "    def __init__(self, train_data, train_target, split_score_function, leaf_value_estimator,\\\n",
    "                 depth=0, min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize the decision tree classifier\n",
    "        \n",
    "        :param train_data: a numpy array of training data, shape = (n, m)\n",
    "        :param train_target: a numpy array of labels, shape = (n, 1)\n",
    "        :param split_score_function: method for splitting node\n",
    "        :param leaf_value_estimator: method for estimating leaf value\n",
    "        :param depth: depth indicator, default value is 0, representing root node\n",
    "        :param min_sample: an internal node can be splitted only if it contains points more than min_smaple\n",
    "        :param max_depth: restriction of tree depth.\n",
    "        '''\n",
    "        self.train = train_data\n",
    "        self.split_score_function = split_score_function\n",
    "        self.leaf_value_estimator = leaf_value_estimator\n",
    "        self.value = leaf_value_estimator(train_target)\n",
    "        self.depth = depth\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    \n",
    "    def split_tree(self, train_data, train_target):\n",
    "        '''\n",
    "        Split the internal node of decision tree\n",
    "        \n",
    "        :param train_data: a numpy array of training data, shape = (n, m)\n",
    "        :param train_label: a numpy array of labels, shape = (n, 1)\n",
    "        \n",
    "        :return split_id: feature used for splitting this node, -1 if splitting cannot decrease score\n",
    "        :return split_value: value used for splitting this node, np.inf if splitting cannot decrease score\n",
    "        '''\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        return split_id, split_value\n",
    "    \n",
    "    \n",
    "    def split_node(self, train_data, train_target):\n",
    "        '''\n",
    "        Splitting the internal node of decision tree.\n",
    "        \n",
    "        :param train_data: a numpy array of training data, shape = (n, m)\n",
    "        :param train_target: a numpy array of labels, shape = (n, 1)\n",
    "        \n",
    "        :return left_index: index of left tree, None value if this node cannot be split\n",
    "        :return right_index: index of right tree, None value if this node cannot be split\n",
    "        '''\n",
    "        \n",
    "        # Your code goes here\n",
    "        \n",
    "        return left_index, right_index\n",
    "\n",
    "    def predict(self, instance):\n",
    "        '''\n",
    "        Predict label by decision tree\n",
    "        \n",
    "        :param instance: a numpy array with new data, shape (1, m)\n",
    "        \n",
    "        :return self.label: predicted label\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        if instance[self.split_id] <= self.split_value:\n",
    "            return self.left.predict(instance)\n",
    "        else:\n",
    "            return self.right.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_entropy(label_array):\n",
    "    '''\n",
    "    Calulate the entropy of given label list\n",
    "    \n",
    "    :param label_array: a numpy array of labels shape = (n, 1)\n",
    "    :return entropy: entropy value\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return entropy\n",
    "\n",
    "def compute_gini(label_array):\n",
    "    '''\n",
    "    Calulate the gini index of label list\n",
    "    \n",
    "    :param label_array: a numpy array of labels shape = (n, 1)\n",
    "    :return gini: gini index value\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common_label(train_target):\n",
    "    '''\n",
    "    Find most common label\n",
    "    '''\n",
    "    label_cnt = Counter(train_target.reshape(len(train_target)))\n",
    "    label = label_cnt.most_common(1)[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DTree_classifier(DTree):\n",
    "    '''\n",
    "    Decision Tree Classifier class, inherit from DTree class\n",
    "    \n",
    "    :attribute criterion_dict: dictionary containing the criterion function\n",
    "    :attribute label_list: a list containing all possible labels\n",
    "    :attribute label_probability: a list containing probability of each possible label\n",
    "    \n",
    "    :method predict_prob: predict label probability vector\n",
    "    '''\n",
    "    \n",
    "    criterion_dict = {\n",
    "        'entropy': compute_entropy,\n",
    "        'gini': compute_gini\n",
    "    }\n",
    "    \n",
    "    def __init__(self, train_data, train_target, criterion='entropy', depth=0, min_sample=5, max_depth=10, label_list=None):\n",
    "        '''\n",
    "        Initialize DTree_classifier object\n",
    "        :param criterion(str): criterion of splitting internal node\n",
    "        :param label_list: a list containing all possible labels\n",
    "        '''\n",
    "        \n",
    "        DTree.__init__(self, train_data, train_target, self.criterion_dict[criterion], most_common_label,\\\n",
    "                      depth=depth, min_sample=min_sample, max_depth=max_depth) # Ininitialize a DTree object\n",
    "        \n",
    "        # collect possible label list\n",
    "        if label_list is None:\n",
    "            label_cnt = Counter(train_target.reshape(len(train_target)))\n",
    "            self.label_list = sorted(label_cnt.keys())\n",
    "        else:\n",
    "            self.label_list = label_list\n",
    "        \n",
    "        # compute label probability vector\n",
    "        self.label_probability = self.compute_probability(train_target) # compute label probability vector\n",
    "            \n",
    "        # split internal node\n",
    "        left_index, right_index = self.split_node(train_data, train_target)\n",
    "\n",
    "        # if the internal node can be splitted, define left and right subtree\n",
    "        if (not (left_index is None)) and (not (right_index is None)):\n",
    "            \n",
    "            self.left = # Your code goes here\n",
    "            self.right = # Your code goes here\n",
    "    \n",
    "    def compute_probability(self, train_target):\n",
    "        '''\n",
    "        Compute probability vector\n",
    "        '''\n",
    "        # Your code goes here\n",
    "        return prob_vector\n",
    "    \n",
    "    def predict_prob(self, instance):\n",
    "        '''\n",
    "        Predict label by decision tree\n",
    "        \n",
    "        :param instance: a numpy array with new data, shape (1, m)\n",
    "        \n",
    "        :return self.label: predicted label\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self.label_probability\n",
    "        if instance[self.split_id] <= self.split_value:\n",
    "            return self.left.predict_prob(instance)\n",
    "        else:\n",
    "            return self.right.predict_prob(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training classifiers with different depth\n",
    "clf1 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=1)\n",
    "clf2 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=2)\n",
    "clf3 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=3)\n",
    "clf4 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=4)\n",
    "clf5 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=5)\n",
    "clf6 = DTree_classifier(x_svm_train, y_svm_train_label, criterion='entropy', max_depth=6)\n",
    "\n",
    "# Plotting decision regions\n",
    "x_min, x_max = x_svm_train[:, 0].min() - 1, x_svm_train[:, 0].max() + 1\n",
    "y_min, y_max = x_svm_train[:, 1].min() - 1, x_svm_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                        [clf1, clf2, clf3, clf4, clf5, clf6],\n",
    "                        ['Depth = {}'.format(n) for n in range(1, 7)]):\n",
    "\n",
    "    Z = np.array([clf.predict(x) for x in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_svm_train[:, 0], x_svm_train[:, 1], c=y_svm_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=5)\n",
    "clf.fit(x_svm_train, y_svm_train_label)\n",
    "export_graphviz(clf, out_file='tree_classifier.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize decision tree\n",
    "!dot -Tpng tree_classifier.dot -o tree_classifier.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='tree_classifier.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_square_error(train_target):\n",
    "    '''\n",
    "    Calulate the mean square error of given target list\n",
    "    \n",
    "    :param train_target: a numpy array of targets shape = (n, 1)\n",
    "    :return mse\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return mse\n",
    "\n",
    "def mean_absolute_error(train_target):\n",
    "    '''\n",
    "    Calulate the mean absolute error of given target list\n",
    "    \n",
    "    :param train_target: a numpy array of targets shape = (n, 1)\n",
    "    :return mae\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_value(train_target):\n",
    "    '''\n",
    "    mean value estimator\n",
    "    '''\n",
    "    return train_target.mean()\n",
    "\n",
    "def median_value(train_target):\n",
    "    '''\n",
    "    median value estimator\n",
    "    '''\n",
    "    return np.median(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DTree_regressor(DTree):\n",
    "    '''\n",
    "    Decision Tree Regressor class, inherit from DTree class\n",
    "    \n",
    "    :attribute criterion_dict: dictionary containing the criterion function\n",
    "    :attribute estimator_dict: dictionary containing the estimation function\n",
    "    '''\n",
    "    \n",
    "    criterion_dict = {\n",
    "        'mse': mean_square_error,\n",
    "        'mae': mean_absolute_error\n",
    "    }\n",
    "    \n",
    "    estimator_dict = {\n",
    "        'mean': mean_value,\n",
    "        'median': median_value\n",
    "    }\n",
    "    \n",
    "    def __init__(self, train_data, train_target, criterion='mse', estimator='mean', depth=0, min_sample=5, max_depth=10):\n",
    "        '''\n",
    "        Initialize DTree_regressor object\n",
    "        :param criterion(str): criterion of splitting internal node\n",
    "        :param estimator(str): value estimator of internal node\n",
    "        '''\n",
    "        \n",
    "        DTree.__init__(self, train_data, train_target, self.criterion_dict[criterion], self.estimator_dict[estimator],\\\n",
    "                      depth=depth, min_sample=min_sample, max_depth=max_depth) # Ininitialize a DTree object\n",
    "            \n",
    "        # split internal node\n",
    "        left_index, right_index = self.split_node(train_data, train_target)\n",
    "\n",
    "        # if the internal node can be splitted, define left and right subtree\n",
    "        if (not (left_index is None)) and (not (right_index is None)):\n",
    "            \n",
    "            self.left = # Your code goes here\n",
    "            self.right = # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pseudo-residual function.\n",
    "#Here you can assume that we are using L2 loss\n",
    "\n",
    "def pseudo_residual_L2(train_target, train_predict):\n",
    "    '''\n",
    "    Compute the pseudo-residual based on current predicted value. \n",
    "    '''\n",
    "    return train_target - train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gradient_boosting():\n",
    "    '''\n",
    "    Gradient Boosting regressor class\n",
    "    :method fit: fitting model\n",
    "    '''\n",
    "    def __init__(self, n_estimator, pseudo_residual_func, learning_rate=0.1, min_sample=5, max_depth=3):\n",
    "        '''\n",
    "        Initialize gradient boosting class\n",
    "        \n",
    "        :param n_estimator: number of estimators\n",
    "        :pseudo_residual_func: function used for computing pseudo-residual\n",
    "        :param learning_rate: step size of gradient descent\n",
    "        '''\n",
    "        self.n_estimator = n_estimator\n",
    "        self.pseudo_residual_func = pseudo_residual_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, train_data, train_target):\n",
    "        '''\n",
    "        Fit gradient boosting model\n",
    "        '''\n",
    "        # Your code goes here \n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        '''\n",
    "        Predict value\n",
    "        '''\n",
    "        # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-D GBM visualization - SVM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = x_svm_train[:, 0].min() - 1, x_svm_train[:, 0].max() + 1\n",
    "y_min, y_max = x_svm_train[:, 1].min() - 1, x_svm_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbt = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbt.fit(x_svm_train, y_svm_train)\n",
    "                   \n",
    "    Z = np.sign(gbt.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(x_svm_train[:, 0], x_svm_train[:, 1], c=y_svm_train_label, alpha=0.8)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D GBM visualization - KRR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_krr_train = np.loadtxt('krr-train.txt')\n",
    "data_krr_test = np.loadtxt('krr-test.txt')\n",
    "\n",
    "x_krr_train, y_krr_train = data_krr_train[:,0].reshape(-1,1),data_krr_train[:,1].reshape(-1,1)\n",
    "x_krr_test, y_krr_test = data_krr_test[:,0].reshape(-1,1),data_krr_test[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_size = 0.001\n",
    "x_range = np.arange(0., 1., plot_size).reshape(-1, 1)\n",
    "\n",
    "f2, axarr2 = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(15, 10))\n",
    "\n",
    "for idx, i, tt in zip(product([0, 1], [0, 1, 2]),\n",
    "                       [1, 5, 10, 20, 50, 100], \n",
    "                       ['n_estimator = {}'.format(n) for n in [1, 5, 10, 20, 50, 100]]):\n",
    "    \n",
    "    gbm_1d = gradient_boosting(n_estimator=i, pseudo_residual_func=pseudo_residual_L2, max_depth=2)  \n",
    "    gbm_1d.fit(x_krr_train, y_krr_train)\n",
    "    \n",
    "    y_range_predict = gbm_1d.predict(x_range)\n",
    "\n",
    "    axarr2[idx[0], idx[1]].plot(x_range, y_range_predict, color='r')\n",
    "    axarr2[idx[0], idx[1]].scatter(x_krr_train, y_krr_train, alpha=0.8)\n",
    "    axarr2[idx[0], idx[1]].set_title(tt)\n",
    "    axarr2[idx[0], idx[1]].set_xlim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
